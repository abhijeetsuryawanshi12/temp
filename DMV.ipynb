{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#P-1\n"
      ],
      "metadata": {
        "id": "qifmASr0um2-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m3EKBOYVuc8I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "csv_data = pd.read_csv('7_Sales_data_csv.csv', encoding=\"cp1252\")\n",
        "print(\"CSV Data:\")\n",
        "print(csv_data.head())\n",
        "\n",
        "excel_data = pd.read_excel('7_Sales-data_excel.xlsx')\n",
        "print(\"\\nExcel Data:\")\n",
        "print(excel_data.head())\n",
        "\n",
        "json_data = pd.read_json('7_Customers_json.json')\n",
        "print(\"\\nJSON Data:\")\n",
        "print(json_data.head())\n",
        "\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(\"CSV missing values:\\n\", csv_data.isnull().sum())\n",
        "print(\"\\nExcel missing values:\\n\", excel_data.isnull().sum())\n",
        "print(\"\\nJSON missing values:\\n\", json_data.isnull().sum())\n",
        "\n",
        "# Step 3: Perform data cleaning (e.g., handling missing values and removing duplicates)\n",
        "\n",
        "# Filling missing values with 0 (this is a simple strategy, you can use more advanced techniques)\n",
        "#csv_data.fillna(0, inplace=True)\n",
        "#excel_data.fillna(0, inplace=True)\n",
        "#json_data.fillna(0, inplace=True)\n",
        "\n",
        "for column in csv_data.select_dtypes(include=np.number).columns:\n",
        "    csv_data[column].fillna(csv_data[column].median(), inplace=True)\n",
        "\n",
        "# Remove duplicates\n",
        "csv_data.drop_duplicates(inplace=True)\n",
        "excel_data.drop_duplicates(inplace=True)\n",
        "json_data.drop_duplicates(inplace=True)\n",
        "\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(\"CSV missing values:\\n\", csv_data.isnull().sum())\n",
        "print(\"\\nExcel missing values:\\n\", excel_data.isnull().sum())\n",
        "print(\"\\nJSON missing values:\\n\", json_data.isnull().sum())\n",
        "\n",
        "# Step 4: Convert data into a unified format (a common dataframe)\n",
        "\n",
        "# For simplicity, let's assume the structures of all data are similar\n",
        "# We'll concatenate the datasets into a single dataframe\n",
        "# Add a source column to know which file the data came from\n",
        "csv_data['source'] = 'csv'\n",
        "excel_data['source'] = 'excel'\n",
        "json_data['source'] = 'json'\n",
        "\n",
        "# Concatenate all datasets into one unified dataframe\n",
        "all_data = pd.concat([csv_data, excel_data, json_data], ignore_index=True)\n",
        "\n",
        "print(\"\\nUnified Data (first 10 rows):\")\n",
        "print(all_data.head(10))\n",
        "\n",
        "all_data.iloc[:,0:15].head()\n",
        "\n",
        "all_data['STATUS'].value_counts()\n",
        "\n",
        "all_data.iloc[10:5,:].head()\n",
        "\n",
        "all_data.columns\n",
        "\n",
        "# Step 5: Perform data transformations\n",
        "# Example: Splitting a 'Product' column into 'Category' and 'Subcategory' if needed\n",
        "# Here, let's assume we have a column 'Product' that we want to split\n",
        "# (You can adjust based on the actual data structure)\n",
        "\n",
        "# Standardize column names (e.g., make all lowercase)\n",
        "all_data.columns = all_data.columns.str.lower()\n",
        "\n",
        "# Derive a new variable: total sales by multiplying 'quantityordered' and 'priceeach'\n",
        "all_data['sales'] = all_data['quantityordered'] * all_data['priceeach']\n",
        "\n",
        "# Assuming the 'Product' column has \"Category-Subcategory\" format, split the columns\n",
        "#if 'status' in all_data.columns:\n",
        "    #all_data[['Shipped', 'Cancelled', 'Resolved', 'On Hold', 'In Process', 'Disputed']] = all_data['status'].str.split('-', expand=True)\n",
        "\n",
        "all_data.info()\n",
        "\n",
        "# Step 6: Analyze the sales data\n",
        "# Example: Calculate total sales and average order value by product category\n",
        "\n",
        "# Calculate total sales\n",
        "total_sales = all_data['sales'].sum()\n",
        "print(\"Total Sales:\", total_sales)\n",
        "\n",
        "# Calculate average order value\n",
        "average_order_value = all_data['sales'].mean()\n",
        "print(\"Average Order Value:\", average_order_value)\n",
        "\n",
        "# Aggregate sales by product line\n",
        "sales_by_productline = all_data.groupby('productline')['sales'].sum()\n",
        "print(\"\\nSales by Product Line:\\n\", sales_by_productline)\n",
        "\n",
        "sales_by_productline.sort_values(ascending = False)\n",
        "\n",
        "# Step 8: Create visualizations\n",
        "\n",
        "# Bar plot of total sales by category\n",
        "plt.figure(figsize=(10, 6))\n",
        "sales_by_productline.plot(kind='bar', color='skyblue')\n",
        "plt.title('Total Sales by Product Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Total Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Pie chart of sales distribution across categories\n",
        "plt.figure(figsize=(8, 8))\n",
        "sales_by_productline.plot(kind='pie', autopct='%1.1f%%', startangle=90, colormap='Set3')\n",
        "plt.title('Sales Distribution by Category')\n",
        "plt.ylabel('')  # Hide the y-label\n",
        "plt.show()\n",
        "\n",
        " # Box plot of sales to identify the spread and potential outliers\n",
        "plt.figure(figsize=(8, 6))\n",
        "all_data.boxplot(column='total_sales', by='productline', grid=True)\n",
        "plt.title('Sales Distribution by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Sales')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Export the unified, cleaned dataset for future use\n",
        "all_data.to_csv('Unified_Sales_Data.csv', index=False)\n",
        "print(\"\\nUnified and cleaned data exported to 'Unified_Sales_Data.csv'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P-2"
      ],
      "metadata": {
        "id": "WFHhiKnl4VQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "\n",
        "# Step 2:  Set up the API key and endpoint for OpenWeatherMap\n",
        "API_KEY = \"1291a2d59057b874e2d705f2462153f2\"  # Replace with your actual API key\n",
        "city = \"Pune\"\n",
        "url = f\"https://api.openweathermap.org/data/2.5/forecast?q={city}&appid={API_KEY}\"\n",
        "\n",
        "# Step 3: Make a request to OpenWeatherMap API and get weather data\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Data retrieval successful!\")\n",
        "    data = response.json()\n",
        "else:\n",
        "    print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "\n",
        "print(data)\n",
        "\n",
        "# Step 4: Extract relevant weather attributes (temperature, humidity, wind speed)\n",
        "# We will parse the 'list' section of the API response that contains the forecast\n",
        "weather_data = []\n",
        "for data in data['list']:\n",
        "    weather_data.append({\n",
        "        \"date\": dt.datetime.fromtimestamp(data['dt']),           # Convert timestamp to date\n",
        "        \"temperature\": data['main']['temp'],                     # Current temperature\n",
        "        \"humidity\": data['main']['humidity'],                    # Humidity\n",
        "        \"wind_speed\": data['wind']['speed'],                     # Wind speed\n",
        "        \"precipitation\": data.get('rain', {}).get('1h', 0)       # Precipitation (if available)\n",
        "})\n",
        "\n",
        "# Step 5: Convert the extracted data into a pandas DataFrame\n",
        "df = pd.DataFrame(weather_data)\n",
        "print(\"\\nFirst few rows of the weather data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 6: Clean and preprocess the data (handling missing values if any)\n",
        "df.fillna(0, inplace=True)  # Fill any missing values with 0\n",
        "print(\"\\nCleaned data:\")\n",
        "print(df.info())\n",
        "\n",
        "# Step 7: Perform data modeling: Calculate daily average temperature\n",
        "df['date_only'] = df['date'].dt.date\n",
        "daily_avg_temp = df.groupby('date_only')['temperature'].mean()\n",
        "\n",
        "# Step 8: Visualize the weather data (temperature trends)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['date'], df['temperature'], color='blue', label='Temperature')\n",
        "plt.title(f'Temperature Trend in {city}')\n",
        "plt.xlabel('Date and Time')\n",
        "plt.ylabel('Temperature (Â°C)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Visualize humidity levels over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['date'], df['humidity'], color='green', label='Humidity')\n",
        "plt.title(f'Humidity Trend in {city}')\n",
        "plt.xlabel('Date and Time')\n",
        "plt.ylabel('Humidity (%)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Scatter plot for correlation between temperature and humidity\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['temperature'], df['humidity'], color='purple')\n",
        "plt.title('Temperature vs Humidity')\n",
        "plt.xlabel('Temperature (Â°C)')\n",
        "plt.ylabel('Humidity (%)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 11: Data Aggregation - Average daily temperature\n",
        "plt.figure(figsize=(10, 6))\n",
        "daily_avg_temp.plot(kind='bar', color='orange')\n",
        "plt.title(f'Daily Average Temperature in {city}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Temperature (Â°C)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Step 12: Export the cleaned data for future use\n",
        "df.to_csv('Cleaned_Weather_Data.csv', index=False)\n",
        "print(\"Cleaned weather data exported to 'Cleaned_Weather_Data.csv'.\")"
      ],
      "metadata": {
        "id": "XMFKcjoY4UYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P-3"
      ],
      "metadata": {
        "id": "cSI9_LGp5Khq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Task 1: Import the dataset\n",
        "df = pd.read_csv('9_telecom_customer_churn.csv')\n",
        "\n",
        "# Task 2: Explore the dataset to understand its structure and content\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "df.describe()\n",
        "\n",
        "# Task 3: Handle missing values\n",
        "# Checking for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Strategy: Fill numerical columns with the median, and categorical columns with the mode\n",
        "for column in df.select_dtypes(include=np.number).columns:\n",
        "    df[column].fillna(df[column].median(), inplace=True)\n",
        "\n",
        "for column in df.select_dtypes(include='object').columns:\n",
        "    df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "\n",
        "# Task 4: Remove duplicate records\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "df['Gender'].unique()\n",
        "\n",
        "# Task 5: Check for inconsistent data and standardize it\n",
        "# For simplicity, let's standardize categorical columns like 'gender', 'Yes', 'No' etc.\n",
        "df['Gender'] = df['Gender'].str.strip().str.lower().replace({'male': 'Male', 'female': 'Female'})\n",
        "df['Married'] = df['Married'].replace({'Yes': 1, 'No': 0})\n",
        "\n",
        "# You can standardize other columns similarly as needed\n",
        "# ...\n",
        "\n",
        "# Task 6: Convert columns to the correct data types\n",
        "# For example, converting 'TotalCharges' column to numeric if it is incorrectly formatted\n",
        "df['Total Charges'] = pd.to_numeric(df['Total Charges'], errors='coerce')\n",
        "\n",
        "# Again, checking for any remaining missing values after type conversion\n",
        "#df['TotalCharges'].fillna(df['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# Task 7: Identify and handle outliers\n",
        "# Let's identify outliers in numerical columns using the IQR method\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Example: Removing outliers from 'MonthlyCharges'\n",
        "df = remove_outliers(df, 'Monthly Charge')\n",
        "\n",
        "# Task 8: Perform feature engineering\n",
        "# Creating a new feature 'Tenure_Group' by binning the 'tenure' column\n",
        "df['Total Revenue Analysis'] = pd.cut(df['Total Revenue'], bins=[0, 500, 1000, 1500, np.inf], labels=['0-500', '500-1000', '1000-1500', '1500+'])\n",
        "\n",
        "# Task 9: Normalize or scale the data\n",
        "# We can normalize continuous features like 'MonthlyCharges' and 'TotalCharges'\n",
        "scaler = StandardScaler()\n",
        "df['Monthly Charge'] = scaler.fit_transform(df[['Monthly Charge']])\n",
        "\n",
        "# Task 10: Split the dataset into training and testing sets\n",
        "X = df.drop(columns=['Customer Status'])  # Features\n",
        "y = df['Customer Status']  # Target variable\n",
        "\n",
        "# Split into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Task 11: Save the cleaned and processed dataset\n",
        "df.to_csv('cleaned_telecom_customer_churn.csv', index=False)\n",
        "\n",
        "print(\"\\nData Cleaning Completed. Cleaned dataset saved as 'cleaned_telecom_customer_churn.csv'\")"
      ],
      "metadata": {
        "id": "_L4OglL0utVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P-4"
      ],
      "metadata": {
        "id": "n1_fxRh3L9WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Task 1: Import dataset and clean column names\n",
        "df = pd.read_csv('10_Bengaluru_House_Data.csv')\n",
        "\n",
        "df.head()\n",
        "\n",
        "# Clean column names by removing spaces and special characters\n",
        "df.columns = df.columns.str.replace(' ', '_').str.replace('[^A-Za-z0-9_]+', '')\n",
        "\n",
        "# Task 2: Handle missing values\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Example strategy: Fill missing numerical values with the mean\n",
        "df['location'].fillna(df['location'].mode()[0], inplace=True)\n",
        "df['size'].fillna(df['size'].mode()[0], inplace=True)\n",
        "df['society'].fillna(df['society'].mode()[0], inplace=True)\n",
        "df['bath'].fillna(df['bath'].mode()[0], inplace=True)\n",
        "df['balcony'].fillna(df['balcony'].mode()[0], inplace=True)\n",
        "\n",
        "# Alternatively, fill categorical missing values with the mode\n",
        "#df.fillna(df.mode().iloc[0], inplace=True)\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Task 3: Perform data merging (if additional datasets are available)\n",
        "# Assuming we have a neighborhood dataset 'neighborhood_data.csv'\n",
        "neighborhood_df = pd.read_csv('Neighborhood.csv')\n",
        "# Merging based on a common column, for example, 'Neighborhood'\n",
        "df = pd.merge(df, neighborhood_df, on='location', how='left')\n",
        "\n",
        "# Task 4: Filter and subset data based on specific criteria\n",
        "# Example: Filter properties sold in a specific year (e.g., 2020) and in a specific location\n",
        "filtered_df = df[(df['size'] == '2 BHK') & (df['location'] == 'Kothanur')]\n",
        "\n",
        "print(\"\\nFiltered data:\")\n",
        "print(filtered_df.head())\n",
        "\n",
        "# Task 5: Handle categorical variables by encoding\n",
        "# Using one-hot encoding for a categorical column, e.g., 'Property_Type'\n",
        "df_encoded = pd.get_dummies(df, columns=['area_type'], drop_first=True)\n",
        "\n",
        "print(\"\\nData after encoding categorical variables:\")\n",
        "print(df_encoded.head())\n",
        "\n",
        "# Task 6: Aggregate data to calculate summary statistics\n",
        "# Example: Calculate average sale price by neighborhood\n",
        "avg_price_by_neighborhood = df.groupby('location')['price'].mean()\n",
        "\n",
        "print(\"\\nAverage Sale Price by Neighborhood:\")\n",
        "print(avg_price_by_neighborhood)\n",
        "\n",
        "# Task 7: Identify and handle outliers\n",
        "# Example: Remove outliers based on Sale Price (values more than 3 standard deviations from the mean)\n",
        "mean_price = df['price'].mean()\n",
        "std_price = df['price'].std()\n",
        "\n",
        "df_no_outliers = df[(df['price'] > (mean_price - 3 * std_price)) & (df['price'] < (mean_price + 3 * std_price))]\n",
        "\n",
        "print(\"\\nData after removing outliers:\")\n",
        "print(df_no_outliers.head())\n",
        "\n",
        "# Task 7: Identify and handle outliers\n",
        "# Example: Remove outliers based on Sale Price (values more than 3 standard deviations from the mean)\n",
        "mean_price = df['price'].mean()\n",
        "std_price = df['price'].std()\n",
        "\n",
        "df_no_outliers = df[(df['price'] > (mean_price - 3 * std_price)) & (df['price'] < (mean_price + 3 * std_price))]\n",
        "\n",
        "print(\"\\nData after removing outliers:\")\n",
        "print(df_no_outliers.head())\n",
        "\n",
        "# Save the cleaned and processed dataset\n",
        "df_no_outliers.to_csv('Cleaned_RealEstate_Prices.csv', index=False)\n",
        "\n",
        "print(\"\\nData Wrangling Completed. Cleaned dataset saved as 'Cleaned_RealEstate_Prices.csv'\")"
      ],
      "metadata": {
        "id": "1TjTGoR5L9CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P-5"
      ],
      "metadata": {
        "id": "yoTlOQkjO3Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Task 1: Import the dataset\n",
        "df = pd.read_csv('12_City_Air_Quality.csv')\n",
        "\n",
        "# Task 2: Explore the dataset structure\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "df.head()\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "for column in df.select_dtypes(include=np.number).columns:\n",
        "  df[column].fillna(df[column].median(), inplace=True)\n",
        "\n",
        "for column in df.select_dtypes(include='object').columns:\n",
        "  df[column].fillna(df[column].mode()[0], inplace=True)\n",
        "\n",
        "# Task 3: Identify relevant variables\n",
        "# We assume columns like 'Date', 'PM2.5', 'PM10', 'CO', and 'AQI' are present in the dataset.\n",
        "# Convert 'Date' column to datetime if necessary\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
        "df['AQI'] = pd.to_numeric(df['AQI'])\n",
        "\n",
        "df.dtypes\n",
        "\n",
        "# Task 4: Create line plot to visualize overall AQI trend over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df['Date'], df['AQI'], color='b', label='AQI')\n",
        "plt.title('AQI Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('AQI')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Task 5: Plot individual pollutant levels over time (PM2.5, PM10, CO)\n",
        "\n",
        "# PM2.5 plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(df['Date'], df['PM2.5'], color='r', label='PM2.5')\n",
        "plt.title('PM2.5 Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('PM2.5')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# PM10 plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(df['Date'], df['PM10'], color='g', label='PM10')\n",
        "plt.title('PM10 Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('PM10')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# CO plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(df['Date'], df['CO'], color='purple', label='CO')\n",
        "plt.title('CO Trend Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('CO')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Task 6: Bar plot to compare AQI values across different dates\n",
        "# Example: Using a subset of the data for specific dates\n",
        "df_subset = df[df['Date'] < '2015-02-10']  # Adjust dates based on your dataset\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df_subset['Date'], df_subset['AQI'], color='orange')\n",
        "plt.title('AQI Comparison Across Dates')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('AQI')\n",
        "plt.show()\n",
        "\n",
        "# Task 7: Box plot to analyze AQI distribution for different pollutants\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.boxplot([df['PM2.5'], df['PM10'], df['CO']], labels=['PM2.5', 'PM10', 'CO'])\n",
        "plt.title('Distribution of Pollutant Levels')\n",
        "plt.ylabel('Levels')\n",
        "plt.show()\n",
        "\n",
        "# Task 8: Scatter plot to explore relationship between AQI and pollutant levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Scatter plot: AQI vs PM2.5\n",
        "plt.scatter(df['PM2.5'], df['AQI'], color='r', label='PM2.5')\n",
        "plt.title('Relationship Between PM2.5 and AQI')\n",
        "plt.xlabel('PM2.5')\n",
        "plt.ylabel('AQI')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "rT1auNOtO39B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#P-6"
      ],
      "metadata": {
        "id": "wGqB_826TBxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Task 1: Import the dataset\n",
        "df = pd.read_csv('12_customer_shopping_data.csv')\n",
        "\n",
        "# Task 2: Explore the dataset structure\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Task 3: Identify relevant variables\n",
        "# Assume relevant columns include 'Shopping Mall', 'Price Amount', 'Product_Category'\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df['sales'] = df['quantity'] * df['price']\n",
        "\n",
        "# Task 4: Group sales data by region and calculate total sales for each region\n",
        "sales_by_region = df.groupby('shopping_mall')['sales'].sum().reset_index()\n",
        "\n",
        "print(\"\\nTotal sales by region:\")\n",
        "print(sales_by_region)\n",
        "\n",
        "# Task 5: Create bar plot to visualize sales distribution by region\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(sales_by_region['shopping_mall'], sales_by_region['sales'], color='skyblue')\n",
        "plt.title('Sales Distribution by Region')\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Total Sales Amount')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Task 6: Identify the top-performing regions based on highest sales\n",
        "top_regions = sales_by_region.sort_values(by='sales', ascending=False)\n",
        "print(\"\\nTop-performing regions:\")\n",
        "print(top_regions)\n",
        "\n",
        "# Task 7: Group sales data by region and product category to calculate total sales\n",
        "sales_by_region_category = df.groupby(['shopping_mall', 'category'])['sales'].sum().unstack()\n",
        "\n",
        "print(\"\\nTotal sales by region and product category:\")\n",
        "print(sales_by_region_category)\n",
        "\n",
        "# Alternatively, create grouped bar plot for the same comparison\n",
        "sales_by_region_category.plot(kind='bar', stacked=False, figsize=(12, 6))\n",
        "plt.title('Grouped Sales by Region and Product Category')\n",
        "plt.xlabel('shopping_mall')\n",
        "plt.ylabel('Total Sales Amount')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Product Category')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "dhdraUN0TDkc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}